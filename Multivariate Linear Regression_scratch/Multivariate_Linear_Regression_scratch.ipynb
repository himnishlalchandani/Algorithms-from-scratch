{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1XslDQHhRbyyLhPtTW06d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import necessary libraries and convert csv to dataframe"
      ],
      "metadata": {
        "id": "8hBGhhuhyKkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XRt0MjTZtsS2"
      },
      "outputs": [],
      "source": [
        "#import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#convert the csv file to a dataframe\n",
        "fuel_df = pd.read_csv('/content/Fuel.csv')\n",
        "#to check if we have successfully obtained a dataframe, we can print the first 5 rows of the dataframe using the head() function\n",
        "#fuel_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Check for NaN values"
      ],
      "metadata": {
        "id": "hf6lDB2pyGad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "the isnull() function can be used to check where all there is a NaN value in the dataframe\n",
        "and then we can run a for loop to check whether a given column has NaN values and how many (over here knowing the count of NaN values won't be of help but no harm in knowing)\n",
        "and we can append the count of NaN values in a list\n",
        "'''\n",
        "NaN_value_count = []                    #creating an empty list\n",
        "for column in fuel_df.columns:          #for loop to iterate over each column\n",
        "  x = fuel_df[column].isnull().sum()    #assigning the number of NaN values in each column to x\n",
        "  NaN_value_count.append(x)             #append x into a list (NaN_value_count)\n",
        "print(NaN_value_count)\n",
        "#[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] this is the output that we get - so there are no NaN values"
      ],
      "metadata": {
        "id": "0zxJJ47YwrQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca78b1e-5f62-43eb-df4b-f1d180f22ad9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking which features are important\n",
        "*   We will keep all the numeric features\n",
        "*   We remove MODELYEAR as all values are 2014. So removing it won't make any difference"
      ],
      "metadata": {
        "id": "bgRtNIsZyUJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#since all the values are 2014, hence we can remove MODELYEAR\n",
        "fuel_df.drop('MODELYEAR', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "swW4XKwnzEW5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoding categorical columns and normalizing the data using mean normalization"
      ],
      "metadata": {
        "id": "98jP6RNJaEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the categorical columns\n",
        "fuel_df['MAKE'] = fuel_df.MAKE.astype('category').cat.codes                   #MAKE\n",
        "fuel_df['MODEL'] = fuel_df.MODEL.astype('category').cat.codes                 #MODEL\n",
        "fuel_df['VEHICLECLASS'] = fuel_df.VEHICLECLASS.astype('category').cat.codes   #VEHICLECLASS\n",
        "fuel_df['TRANSMISSION'] = fuel_df.TRANSMISSION.astype('category').cat.codes   #TRANSMISSION\n",
        "fuel_df['FUELTYPE'] = fuel_df.FUELTYPE.astype('category').cat.codes           #FUELTYPE\n",
        "\n",
        "#we dont want to normalize the CO2EMISSIONS column hence first remove it\n",
        "fuel_co2emissions = fuel_df.pop('CO2EMISSIONS')\n",
        "\n",
        "#now we normalize the data using mean normalization method\n",
        "#(value - mean)/(standard deviation)\n",
        "for column in fuel_df.columns:\n",
        "  fuel_df[column] = (fuel_df[column] - fuel_df[column].mean())/fuel_df[column].std()\n",
        "\n",
        "#now we will concatenate the CO2EMISSIONS column and the normalized dataframe\n",
        "fuel_df = pd.concat([fuel_df, fuel_co2emissions], axis = 1)   #axis = 1 because we want the dataframes side by side\n",
        "\n",
        "#fuel_df"
      ],
      "metadata": {
        "id": "tpsGrYmIaIzQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I will be using both, the normal equation method and gradient descent to obtain predictions. But the normal equation method is better because of the following reasons\n",
        "*   Number of features is very less\n",
        "*   No need to choose learning rate\n",
        "*   Don't need to solve iteratively using gradient descent\n",
        "\n"
      ],
      "metadata": {
        "id": "VLgo1CQhbD5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Normal Equation Method\n",
        "###Now how does it work?\n",
        "The hypothesis function in general is of the form $h_{\\theta} = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + .... + \\theta_{n}x_{n}$ where $x_{i}$ represents the feature values of each row and $\\theta_{i}$ are the parameters corresponding to each feature. So we basically have to find the values of $\\theta_{i}$ (which I have done using the normal equation method) and then multiply these with the data that is there in y_dev\n",
        " to get the predictions"
      ],
      "metadata": {
        "id": "8FWk86Cfb3t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first we create a column of ones (which corresponds to the x0 of the hypothesis function)\n",
        "fuel_df['x0'] = 1      #lets call the column 'x0'\n",
        "#this will be added at the end\n",
        "#as per the hypothesis function, x0 has to be the first column so we rearrange the columns\n",
        "fuel_df = fuel_df.iloc[:,[12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]"
      ],
      "metadata": {
        "id": "fqe-78wBYntK"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Splitting the dataset into training and development datasets"
      ],
      "metadata": {
        "id": "oAbSi95sXU6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now we split the data into training and development as per a ratio (to determine the number of rows that will be in the training set and the rest in the development set)\n",
        "ratio = 0.7\n",
        "\n",
        "#we do this by creating a list of 1067*ratio randomly selected numbers from a range of 0 to 1067 and assign it to the list rand_list\n",
        "#the elements of rand_list essentially now correspond to the index of a row in fuel_df\n",
        "rand_list = np.random.choice(np.arange(1067), size=int(1067*ratio), replace=False)\n",
        "\n",
        "#next we create x_train. we take each element or rand_list and fill it into x_train\n",
        "x_train = fuel_df.iloc[rand_list]\n",
        "\n",
        "#now we have to fill the remaining elements into x_dev\n",
        "#to do this, we create remaining_list, which contains all those values in the range of 0 to 1067 which are not there in rand_list\n",
        "#this can be done using the setdiff1d() function\n",
        "remaining_list = np.setdiff1d(np.arange(1067), rand_list)\n",
        "\n",
        "#next we create x_dev. we take each element of rand_list and fill it into x_dev\n",
        "x_dev = fuel_df.iloc[remaining_list]\n",
        "\n",
        "#now we create a seperate dataframe for the target variable (CO2EMISSIONS)\n",
        "#just remove the CO2EMISSIONS columns from x_train and x_dev and call it y_train and y_dev\n",
        "y_train = x_train.pop('CO2EMISSIONS')\n",
        "y_dev = x_dev.pop('CO2EMISSIONS')"
      ],
      "metadata": {
        "id": "HIZ-w5aRXbQU"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve for the $\\theta_{i}$ values we use the equation $\\theta = (X^{T}X)^{-1}X^{T}Y$ where $X$ is the matrix of the values present in x_train, $X^{T}$ is the transpose of $X$, and $Y$ is the values of y_train"
      ],
      "metadata": {
        "id": "R5eiO-JLfVOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we have to create the required matrices from the dataframes that we have. This can be done using the .copy() function\n",
        "X = x_train.copy()                         # X\n",
        "XT = X.T                                   # X^T (X transpose)\n",
        "Y = y_train.copy()                         # Y\n",
        "theta_normal_equation = np.linalg.inv(XT @ X) @ XT @ Y     # Theta\n",
        "#now we have the values of the parameters theta0, theta1, ...., theta11"
      ],
      "metadata": {
        "id": "ruocssYcZQ8h"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Make the Predictions"
      ],
      "metadata": {
        "id": "BRGFsc0IhDkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we can make the predictions by taking the dot product of x_dev and theta\n",
        "predictions_normal_equation = np.dot(x_dev, theta_normal_equation)"
      ],
      "metadata": {
        "id": "FaizJjr6hF7s"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using R-squared to measure the goodness of the fit\n",
        "##$R-squared = 1 - \\frac{TSS}{RSS}$\n",
        "where TSS is Total sum of squares and RSS is Residual sum of squares"
      ],
      "metadata": {
        "id": "Ip6BfPgIhm-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TSS is the sum of squares of the differences of the values in y_dev from the mean of y_dev\n",
        "TSS = ((y_dev - y_dev.mean())**2).sum()\n",
        "#TSS is the sum of squares of the differences of the values in y_dev and predictions\n",
        "RSS = ((y_dev - predictions_normal_equation)**2).sum()\n",
        "#calculate r_squared\n",
        "r_squared_normal_equation = 1 - (RSS/TSS)\n",
        "print(r_squared_normal_equation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-HlIemTipBC",
        "outputId": "80f9b000-8e6b-4a93-a6d9-e67dfb9aad50"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9078999605079476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Gradient Descent Method\n",
        "The gradient descent algorithm is of the form  \n",
        "\n",
        "Repeat until convergence {\n",
        "  \n",
        "  $\\theta_{j} := \\theta_{j} - \\alpha * 1/m * \\displaystyle\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
        "  \n",
        "  } Simultaneously update $\\theta_{j}$ for j = 0, 1, ..., n\n",
        "\n",
        "  n = number of features\n",
        "\n",
        "  m = number of rows of data\n",
        "\n",
        "  $\\alpha$ = learning rate\n",
        "\n",
        "  $h_{\\theta}$ = hypothesis function"
      ],
      "metadata": {
        "id": "eHxhjNvvwvdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning values to hyperparameters\n",
        "number_of_iterations = 900       #number_of_iterations\n",
        "alpha = 0.06                     #alpha\n",
        "\n",
        "#m is the length of y_train\n",
        "m = len(y_train)\n",
        "#initialize theta0, theta1, ...., theta11 to zero (can take any value. taking zero for simplicity)\n",
        "theta_gradient_descent = np.zeros(x_train.shape[1])\n",
        "\n",
        "#now we loop over the gradient descent algorithm \"number_of_iterations\" times\n",
        "for i in range (number_of_iterations):\n",
        "  theta_gradient_descent = theta_gradient_descent - alpha * (1/m) * XT @ (X @ theta_gradient_descent - y_train)\n",
        "\n",
        "#we can make the predictions by taking the dot product of x_dev and theta\n",
        "predictions_gradient_descent = np.dot(x_dev, theta_gradient_descent)\n",
        "\n",
        "#now we calculate r-squared for gradient descent\n",
        "TSS = ((y_dev - y_dev.mean())**2).sum()\n",
        "RSS = ((y_dev - predictions_gradient_descent)**2).sum()\n",
        "r_squared_gradient_descent = 1 - (RSS/TSS)\n",
        "print(r_squared_gradient_descent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhjvL09wx8M",
        "outputId": "c40d90df-4188-487a-af1b-f30b661c2e03"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9086945003761459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparing with sklearn"
      ],
      "metadata": {
        "id": "TNXeDZC6nAB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#make the model\n",
        "model = LinearRegression()\n",
        "\n",
        "#train the model\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "#make the predictions\n",
        "predictions_skl = model.predict(x_dev)\n",
        "\n",
        "#measure the goodness of the fit\n",
        "r_squared_sklearn = r2_score(y_dev, predictions_skl)\n",
        "print(r_squared_sklearn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF1_agI2nCYg",
        "outputId": "6f648cbe-9a50-4e71-9931-bdfa0bdc7e11"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9078999605079021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"R-squared: Normal Equation = {r_squared_normal_equation}\")\n",
        "print(f\"R-squared: Gradient Descent = {r_squared_gradient_descent}\")\n",
        "print(f\"R-squared: Sklearn = {r_squared_sklearn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6C0ILiM37gA",
        "outputId": "a7fb3186-01cc-4628-94f7-4a70db126f83"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: Normal Equation = 0.9078999605079476\n",
            "R-squared: Gradient Descent = 0.9086945003761459\n",
            "R-squared: Sklearn = 0.9078999605079021\n"
          ]
        }
      ]
    }
  ]
}